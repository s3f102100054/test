import openai
import json
import streamlit as st
from st_audiorec import st_audiorec
from tempfile import NamedTemporaryFile
from openai import OpenAI
import io
import time

st.set_page_config(page_title="å±…ä½è€…ã‚µãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ", page_icon="./image/DALLÂ·E 2023-12-13.png")

if "all_text" not in st.session_state:
    st.session_state.all_text = []

with st.sidebar:
    st.markdown("## å±…ä½è€…ã‚µãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ \n# ğŸ¤–ã„ãˆãªã³ãã‚“ğŸ¤–")

    api_key = st.text_input("OPEN_AI_KEY", type="password")
    mode = st.selectbox("ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠ", options=["ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "éŸ³å£°å…¥åŠ›"])

if api_key:
    openai.api_key = api_key
    client = OpenAI(
    api_key=api_key
)

    thread = client.beta.threads.create()
    fileidlist = []

    if mode =="ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›":
        user_prompt = st.chat_input("ã‚ãªãŸã®è³ªå•:")
        assistant_text = ""

        for text_info in st.session_state.all_text:
            with st.chat_message(text_info["role"], avatar=text_info["role"]):
                st.write(text_info["role"] + ":\n\n" + text_info["content"])

        if user_prompt:
            with st.chat_message("ã‚ãªãŸ", avatar="user"):
                    st.write("ã‚ãªãŸ" + ":\n\n" + user_prompt)
            st.session_state.all_text.append({
                "role": "ã‚ãªãŸ",
                "content": user_prompt,
                "avatar": "user" 
            })

            if len(st.session_state.all_text) > 10:
                st.session_state.all_text.pop(1)

            # Assistants APIã‚’ä½¿ã£ã¦å›ç­”ã‚’å–å¾—
            messages = client.beta.threads.messages.create(
                thread_id=thread.id,
                role="user",
                content=user_prompt
            )

            run = client.beta.threads.runs.create(
                thread_id=thread.id,
                assistant_id="asst_qE2cw2uUTwGpF4HADzrnAxQm"
            )

            while run.status != "completed":
                run = client.beta.threads.runs.retrieve(
                    thread_id=thread.id,
                    run_id=run.id
                )
                time.sleep(1)

            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ™‚ç³»åˆ—ã«ä¸¦ã¹ã¦è¡¨ç¤º
            message_list = client.beta.threads.messages.list(thread_id=thread.id)
            sorted_messages = sorted(message_list.data, key=lambda x: x.created_at)

            # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã‹ã‚‰ã®æœ€æ–°ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
            for message in sorted_messages:
                if message.role == "assistant":
                    for content_item in message.content:
                        if content_item.type == 'text':
                            assistant_text = content_item.text.value
                            with st.chat_message("ã„ãˆãªã³ãã‚“", avatar="./image/DALLÂ·E 2023-12-13.png"):
                                    st.write("ã„ãˆãªã³ãã‚“" + ":\n\n" + assistant_text)
                            break

            st.session_state.all_text.append(
                {"role": "ã„ãˆãªã³ãã‚“", "content": assistant_text,"avatar": "./image/DALLÂ·E 2023-12-13.png"}
            )

    elif mode == "éŸ³å£°å…¥åŠ›":
        wav_audio_data = st_audiorec()
        if st.button("è³ªå•ã‚’é€ä¿¡"):
            if wav_audio_data:
                # éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
                with NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
                    temp_file.write(wav_audio_data)
                    temp_file.flush()

                    with open(temp_file.name, "rb") as audio_file:
                        transcript = openai.audio.transcriptions.create(
                            model="whisper-1",
                            file=audio_file,
                            response_format="text",
                        )
                        user_prompt = transcript
                        if user_prompt:
                            with st.chat_message("user", avatar="user"):
                                st.write("ã‚ãªãŸ" + ":\n\n" + user_prompt)

                # Assistants APIã‚’ä½¿ã£ã¦å›ç­”ã‚’å–å¾—
                messages = client.beta.threads.messages.create(
                    thread_id=thread.id,
                    role="user",
                    content=user_prompt
                )

                # ã‚¹ãƒ¬ãƒƒãƒ‰ã®å®Ÿè¡Œ
                run = client.beta.threads.runs.create(
                    thread_id=thread.id,
                    assistant_id="asst_qE2cw2uUTwGpF4HADzrnAxQm"
                )

                # ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèª
                while run.status != "completed":
                    run = client.beta.threads.runs.retrieve(
                        thread_id=thread.id,
                        run_id=run.id
                    )
                    time.sleep(1)

                messages = client.beta.threads.messages.list(thread_id=thread.id)
                for message in messages:
                    if message.role == "assistant":
                        for content_item in message.content:
                            if content_item.type == 'text':
                                assistant_text = content_item.text.value
                                with st.chat_message("assistant", avatar="./image/DALLÂ·E 2023-12-13.png"):
                                    st.write("ã„ãˆãªã³ãã‚“" + ":\n\n" + assistant_text)
                                break

                # å›ç­”ã‚’éŸ³å£°ã«å¤‰æ›
                audio_response = openai.audio.speech.create(
                    model="tts-1",
                    voice="nova",
                    input=assistant_text,
                )

                audio_data = audio_response.content
                byte_stream = io.BytesIO(audio_data)
                st.audio(byte_stream)

else:
    st.info("ğŸ‘ˆOPEN_AI_KEYã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
